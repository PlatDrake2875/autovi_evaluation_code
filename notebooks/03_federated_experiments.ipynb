{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated PatchCore Experiments\n",
    "\n",
    "This notebook runs federated learning experiments comparing:\n",
    "1. **IID Partitioning**: Data uniformly distributed across clients\n",
    "2. **Category-Based Partitioning**: Non-IID distribution simulating factory stations\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.autovi_dataset import AutoVIDataset, CATEGORIES\n",
    "from src.data.partitioner import IIDPartitioner, CategoryPartitioner, compute_partition_stats\n",
    "from src.data.preprocessing import get_transforms\n",
    "from src.federated import FederatedPatchCore\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Configuration\n",
    "DATA_ROOT = \"/path/to/autovi\"  # <-- UPDATE THIS PATH\n",
    "OUTPUT_DIR = project_root / \"outputs\" / \"federated\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "dataset = AutoVIDataset(\n",
    "    root_dir=DATA_ROOT,\n",
    "    categories=CATEGORIES,\n",
    "    split=\"train\",\n",
    "    transform=None,\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "stats = dataset.get_statistics()\n",
    "print(f\"\\nSamples per category:\")\n",
    "for cat, counts in stats['by_category'].items():\n",
    "    print(f\"  {cat}: {counts['good']} (all good for training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Partitioning Strategies\n",
    "\n",
    "Let's visualize the data distribution for IID vs Category-based partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partitioners\n",
    "iid_partitioner = IIDPartitioner(num_clients=5, seed=SEED)\n",
    "category_partitioner = CategoryPartitioner(seed=SEED)\n",
    "\n",
    "# Create partitions\n",
    "iid_partition = iid_partitioner.partition(dataset)\n",
    "category_partition = category_partitioner.partition(dataset)\n",
    "\n",
    "# Compute statistics\n",
    "iid_stats = compute_partition_stats(dataset, iid_partition)\n",
    "category_stats = compute_partition_stats(dataset, category_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_partition_distribution(stats, title):\n",
    "    \"\"\"Plot the distribution of categories across clients.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    clients = list(stats['clients'].keys())\n",
    "    categories = list(CATEGORIES)\n",
    "    \n",
    "    x = np.arange(len(clients))\n",
    "    width = 0.12\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        counts = [stats['clients'][c]['by_category'].get(cat, 0) for c in clients]\n",
    "        ax.bar(x + i * width, counts, width, label=cat, color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel('Client ID')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x + width * 2.5)\n",
    "    ax.set_xticklabels([f'Client {c}' for c in clients])\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot both distributions\n",
    "fig1 = plot_partition_distribution(iid_stats, 'IID Partitioning')\n",
    "plt.show()\n",
    "\n",
    "fig2 = plot_partition_distribution(category_stats, 'Category-Based Partitioning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryTransformDataset:\n",
    "    \"\"\"Wrapper to apply category-specific transforms.\"\"\"\n",
    "    def __init__(self, dataset, transforms_dict):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        category = item[\"category\"]\n",
    "        if category in self.transforms:\n",
    "            item[\"image\"] = self.transforms[category](item[\"image\"])\n",
    "        return item\n",
    "\n",
    "\n",
    "class TransformedSubset:\n",
    "    \"\"\"Subset wrapper for transformed dataset.\"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "\n",
    "def create_dataloaders(transformed_dataset, partition, batch_size=32, num_workers=4):\n",
    "    \"\"\"Create dataloaders for each client.\"\"\"\n",
    "    def collate_fn(batch):\n",
    "        images = torch.stack([item[\"image\"] for item in batch])\n",
    "        labels = torch.tensor([item[\"label\"] for item in batch])\n",
    "        categories = [item[\"category\"] for item in batch]\n",
    "        return {\"image\": images, \"label\": labels, \"category\": categories}\n",
    "\n",
    "    dataloaders = {}\n",
    "    for client_id, indices in partition.items():\n",
    "        subset = TransformedSubset(transformed_dataset, indices)\n",
    "        loader = DataLoader(\n",
    "            subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        dataloaders[client_id] = loader\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "# Build transforms\n",
    "transforms_dict = {}\n",
    "for cat in CATEGORIES:\n",
    "    transforms_dict[cat] = get_transforms(cat, normalize=True, to_tensor=True)\n",
    "\n",
    "# Create transformed dataset\n",
    "transformed_dataset = CategoryTransformDataset(dataset, transforms_dict)\n",
    "print(\"Transforms ready for all categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 1: IID Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for IID partition\n",
    "iid_dataloaders = create_dataloaders(transformed_dataset, iid_partition, batch_size=32)\n",
    "\n",
    "print(f\"Created {len(iid_dataloaders)} dataloaders for IID experiment\")\n",
    "for client_id, loader in iid_dataloaders.items():\n",
    "    print(f\"  Client {client_id}: {len(loader.dataset)} samples, {len(loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IID federated model\n",
    "iid_federated = FederatedPatchCore(\n",
    "    num_clients=5,\n",
    "    backbone_name=\"wide_resnet50_2\",\n",
    "    layers=[\"layer2\", \"layer3\"],\n",
    "    coreset_ratio=0.1,\n",
    "    global_bank_size=10000,\n",
    "    neighborhood_size=3,\n",
    "    aggregation_strategy=\"federated_coreset\",\n",
    "    weighted_by_samples=True,\n",
    "    use_faiss=True,\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "# Store partition info\n",
    "iid_federated.partition = iid_partition\n",
    "iid_federated.partition_stats = iid_stats\n",
    "\n",
    "print(iid_federated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run IID federated training\n",
    "print(\"Starting IID federated training...\")\n",
    "iid_global_bank = iid_federated.train(iid_dataloaders, seed=SEED)\n",
    "\n",
    "print(f\"\\nIID Global memory bank: {iid_global_bank.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IID results\n",
    "iid_output_dir = OUTPUT_DIR / \"iid\"\n",
    "iid_federated.save(str(iid_output_dir))\n",
    "print(f\"Saved IID model to {iid_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: Category-Based Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for category partition\n",
    "category_dataloaders = create_dataloaders(transformed_dataset, category_partition, batch_size=32)\n",
    "\n",
    "print(f\"Created {len(category_dataloaders)} dataloaders for Category experiment\")\n",
    "for client_id, loader in category_dataloaders.items():\n",
    "    print(f\"  Client {client_id}: {len(loader.dataset)} samples, {len(loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Category-based federated model\n",
    "category_federated = FederatedPatchCore(\n",
    "    num_clients=5,\n",
    "    backbone_name=\"wide_resnet50_2\",\n",
    "    layers=[\"layer2\", \"layer3\"],\n",
    "    coreset_ratio=0.1,\n",
    "    global_bank_size=10000,\n",
    "    neighborhood_size=3,\n",
    "    aggregation_strategy=\"federated_coreset\",\n",
    "    weighted_by_samples=True,\n",
    "    use_faiss=True,\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "# Store partition info\n",
    "category_federated.partition = category_partition\n",
    "category_federated.partition_stats = category_stats\n",
    "\n",
    "print(category_federated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Category-based federated training\n",
    "print(\"Starting Category-based federated training...\")\n",
    "category_global_bank = category_federated.train(category_dataloaders, seed=SEED)\n",
    "\n",
    "print(f\"\\nCategory Global memory bank: {category_global_bank.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Category results\n",
    "category_output_dir = OUTPUT_DIR / \"category_based\"\n",
    "category_federated.save(str(category_output_dir))\n",
    "print(f\"Saved Category model to {category_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"Comparison Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iid_stats_final = iid_federated.get_stats()\n",
    "category_stats_final = category_federated.get_stats()\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'IID':<15} {'Category':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Global bank size':<30} {iid_stats_final['actual_global_bank_size']:<15} {category_stats_final['actual_global_bank_size']:<15}\")\n",
    "print(f\"{'Feature dimension':<30} {iid_stats_final['feature_dim']:<15} {category_stats_final['feature_dim']:<15}\")\n",
    "\n",
    "if 'training' in iid_stats_final:\n",
    "    iid_time = iid_stats_final['training'].get('elapsed_time_seconds', 'N/A')\n",
    "    category_time = category_stats_final['training'].get('elapsed_time_seconds', 'N/A')\n",
    "    print(f\"{'Training time (s)':<30} {iid_time:<15.2f} {category_time:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client contribution analysis\n",
    "def plot_client_contributions(stats, title):\n",
    "    \"\"\"Plot client contributions to global memory bank.\"\"\"\n",
    "    if 'training' not in stats:\n",
    "        print(\"No training stats available\")\n",
    "        return\n",
    "    \n",
    "    client_stats = stats['training'].get('client_stats', [])\n",
    "    if not client_stats:\n",
    "        print(\"No client stats available\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Samples per client\n",
    "    client_ids = [s['client_id'] for s in client_stats]\n",
    "    num_samples = [s['num_samples'] for s in client_stats]\n",
    "    coreset_sizes = [s['coreset_size'] for s in client_stats]\n",
    "    \n",
    "    axes[0].bar(client_ids, num_samples, color='steelblue')\n",
    "    axes[0].set_xlabel('Client ID')\n",
    "    axes[0].set_ylabel('Number of Samples')\n",
    "    axes[0].set_title(f'{title} - Samples per Client')\n",
    "    \n",
    "    axes[1].bar(client_ids, coreset_sizes, color='coral')\n",
    "    axes[1].set_xlabel('Client ID')\n",
    "    axes[1].set_ylabel('Coreset Size')\n",
    "    axes[1].set_title(f'{title} - Coreset per Client')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_client_contributions(iid_stats_final, 'IID')\n",
    "plt.show()\n",
    "\n",
    "plot_client_contributions(category_stats_final, 'Category-Based')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = AutoVIDataset(\n",
    "    root_dir=DATA_ROOT,\n",
    "    categories=CATEGORIES,\n",
    "    split=\"test\",\n",
    "    transform=None,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
    "test_stats = test_dataset.get_statistics()\n",
    "print(f\"Good: {test_stats['by_label'][0]}, Defective: {test_stats['by_label'][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a few samples\n",
    "def test_inference(model, dataset, num_samples=5):\n",
    "    \"\"\"Run inference on a few test samples.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        image = sample['image']\n",
    "        category = sample['category']\n",
    "        label = sample['label']\n",
    "        \n",
    "        # Apply transforms\n",
    "        transform = get_transforms(category, normalize=True, to_tensor=True)\n",
    "        image_tensor = transform(image).unsqueeze(0)\n",
    "        \n",
    "        # Predict\n",
    "        anomaly_map, image_score = model.predict_single(image_tensor)\n",
    "        \n",
    "        results.append({\n",
    "            'index': i,\n",
    "            'category': category,\n",
    "            'label': 'defective' if label == 1 else 'good',\n",
    "            'score': image_score,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test both models\n",
    "print(\"IID Model Predictions:\")\n",
    "iid_results = test_inference(iid_federated, test_dataset, num_samples=10)\n",
    "for r in iid_results:\n",
    "    print(f\"  Sample {r['index']}: {r['category']}, {r['label']}, score={r['score']:.4f}\")\n",
    "\n",
    "print(\"\\nCategory Model Predictions:\")\n",
    "category_results = test_inference(category_federated, test_dataset, num_samples=10)\n",
    "for r in category_results:\n",
    "    print(f\"  Sample {r['index']}: {r['category']}, {r['label']}, score={r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Partitioning**: Comparing IID (uniform) vs Category-based (non-IID) distributions\n",
    "2. **Federated Training**: Running PatchCore in a federated setting with 5 clients\n",
    "3. **Memory Bank Aggregation**: Using the federated coreset strategy\n",
    "4. **Results Comparison**: Analyzing client contributions and model statistics\n",
    "\n",
    "### Key Observations:\n",
    "- IID partitioning results in uniform data distribution across clients\n",
    "- Category-based partitioning simulates realistic factory scenarios\n",
    "- Both approaches produce comparable global memory banks\n",
    "- Full evaluation requires running the evaluation pipeline on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
