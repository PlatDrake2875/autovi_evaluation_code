{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Results Analysis\n",
    "\n",
    "This notebook analyzes and compares evaluation results from centralized and federated PatchCore models on the AutoVI dataset.\n",
    "\n",
    "## Contents\n",
    "1. Load Evaluation Results\n",
    "2. Per-Object Performance Analysis\n",
    "3. Method Comparison\n",
    "4. Statistical Significance Testing\n",
    "5. Visualizations\n",
    "6. Key Findings & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data.autovi_dataset import CATEGORIES\n",
    "from src.evaluation.visualization import (\n",
    "    plot_fpr_spro_curves,\n",
    "    plot_comparison_bar_chart,\n",
    "    plot_performance_heatmap,\n",
    "    plot_box_comparison,\n",
    "    compute_statistical_analysis,\n",
    "    create_comparison_table,\n",
    ")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Categories: {CATEGORIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "METRICS_DIR = project_root / \"outputs\" / \"evaluation\" / \"metrics\"\n",
    "METHODS = [\"centralized\", \"federated_iid\", \"federated_category\"]\n",
    "FPR_LIMIT = 0.05  # Default FPR limit for analysis\n",
    "\n",
    "print(f\"Metrics directory: {METRICS_DIR}\")\n",
    "print(f\"Methods to analyze: {METHODS}\")\n",
    "print(f\"FPR limit: {FPR_LIMIT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(metrics_dir, methods):\n",
    "    \"\"\"Load metrics from JSON files.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        method_dir = metrics_dir / method\n",
    "        if not method_dir.exists():\n",
    "            print(f\"Warning: {method_dir} not found\")\n",
    "            continue\n",
    "            \n",
    "        method_results = {}\n",
    "        for obj_name in CATEGORIES:\n",
    "            metrics_path = method_dir / obj_name / \"metrics.json\"\n",
    "            if metrics_path.exists():\n",
    "                with open(metrics_path) as f:\n",
    "                    method_results[obj_name] = json.load(f)\n",
    "        \n",
    "        if method_results:\n",
    "            results[method] = method_results\n",
    "            print(f\"Loaded {method}: {len(method_results)} objects\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load results\n",
    "results = load_metrics(METRICS_DIR, METHODS)\n",
    "print(f\"\\nLoaded results for {len(results)} methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Per-Object Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "df = create_comparison_table(results, FPR_LIMIT)\n",
    "print(f\"\\nComparison Table (AUC-sPRO @ FPR={FPR_LIMIT})\")\n",
    "df.style.format(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values for analysis\n",
    "def extract_auc_spro(results, method, fpr_limit=0.05):\n",
    "    \"\"\"Extract AUC-sPRO values for a method.\"\"\"\n",
    "    values = {}\n",
    "    for obj, obj_results in results.get(method, {}).items():\n",
    "        if \"error\" not in obj_results:\n",
    "            auc_spro = obj_results.get(\"localization\", {}).get(\"auc_spro\", {})\n",
    "            val = auc_spro.get(str(fpr_limit))\n",
    "            if val is not None:\n",
    "                values[obj] = val\n",
    "    return values\n",
    "\n",
    "# Get values for each method\n",
    "method_values = {method: extract_auc_spro(results, method, FPR_LIMIT) for method in METHODS if method in results}\n",
    "\n",
    "for method, values in method_values.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    for obj, val in sorted(values.items()):\n",
    "        print(f\"  {obj}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aggregate statistics\n",
    "print(\"Aggregate Statistics (AUC-sPRO @ FPR=0.05)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method, values in method_values.items():\n",
    "    if values:\n",
    "        vals = list(values.values())\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  Mean:   {np.mean(vals):.4f}\")\n",
    "        print(f\"  Std:    {np.std(vals):.4f}\")\n",
    "        print(f\"  Min:    {np.min(vals):.4f}\")\n",
    "        print(f\"  Max:    {np.max(vals):.4f}\")\n",
    "        print(f\"  Median: {np.median(vals):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance gap analysis\n",
    "if \"centralized\" in method_values:\n",
    "    print(\"\\nPerformance Gap Analysis (vs Centralized)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    centralized_vals = method_values[\"centralized\"]\n",
    "    \n",
    "    for method in [\"federated_iid\", \"federated_category\"]:\n",
    "        if method in method_values:\n",
    "            method_vals = method_values[method]\n",
    "            \n",
    "            # Calculate gaps for common objects\n",
    "            common_objs = set(centralized_vals.keys()) & set(method_vals.keys())\n",
    "            gaps = []\n",
    "            for obj in common_objs:\n",
    "                gap = method_vals[obj] - centralized_vals[obj]\n",
    "                gap_pct = gap / centralized_vals[obj] * 100\n",
    "                gaps.append(gap_pct)\n",
    "                print(f\"  {obj}: {gap_pct:+.2f}%\")\n",
    "            \n",
    "            print(f\"\\n  Mean gap ({method}): {np.mean(gaps):+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full statistical analysis\n",
    "stats_analysis = compute_statistical_analysis(results, FPR_LIMIT)\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(\"Descriptive Statistics\")\n",
    "print(\"=\"*50)\n",
    "for method, desc in stats_analysis.get(\"descriptive\", {}).items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    for key, value in desc.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display pairwise comparisons\n",
    "print(\"\\nPairwise Statistical Comparisons\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for comp_key, comp_data in stats_analysis.get(\"comparisons\", {}).items():\n",
    "    print(f\"\\n{comp_key.replace('_vs_', ' vs ')}:\")\n",
    "    print(f\"  Mean difference: {comp_data['mean_diff']:.4f}\")\n",
    "    print(f\"  Cohen's d (effect size): {comp_data['cohens_d']:.4f}\")\n",
    "    \n",
    "    t_test = comp_data.get(\"paired_t_test\", {})\n",
    "    print(f\"  Paired t-test: t={t_test.get('statistic', 0):.4f}, p={t_test.get('p_value', 0):.4f}\")\n",
    "    \n",
    "    wilcoxon = comp_data.get(\"wilcoxon\", {})\n",
    "    if wilcoxon.get(\"p_value\") is not None:\n",
    "        print(f\"  Wilcoxon test: W={wilcoxon.get('statistic', 0):.4f}, p={wilcoxon.get('p_value', 0):.4f}\")\n",
    "    \n",
    "    # Significance interpretation\n",
    "    p_val = t_test.get('p_value', 1)\n",
    "    if p_val < 0.001:\n",
    "        sig = \"*** (p < 0.001)\"\n",
    "    elif p_val < 0.01:\n",
    "        sig = \"** (p < 0.01)\"\n",
    "    elif p_val < 0.05:\n",
    "        sig = \"* (p < 0.05)\"\n",
    "    else:\n",
    "        sig = \"n.s. (not significant)\"\n",
    "    print(f\"  Significance: {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar comparison chart\n",
    "fig = plot_comparison_bar_chart(results, FPR_LIMIT, figsize=(14, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance heatmap\n",
    "fig = plot_performance_heatmap(results, FPR_LIMIT, figsize=(12, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot comparison\n",
    "fig = plot_box_comparison(results, FPR_LIMIT, figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPR-sPRO curves for each object\n",
    "for obj_name in CATEGORIES:\n",
    "    obj_results = {\n",
    "        method: method_results.get(obj_name, {})\n",
    "        for method, method_results in results.items()\n",
    "    }\n",
    "    \n",
    "    # Check if we have data for this object\n",
    "    if any(obj_results.values()):\n",
    "        fig = plot_fpr_spro_curves(obj_results, obj_name, figsize=(8, 6))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Defect Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by defect type (structural vs logical)\n",
    "print(\"Performance by Defect Type\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method, method_results in results.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    \n",
    "    structural_vals = []\n",
    "    logical_vals = []\n",
    "    \n",
    "    for obj_name, obj_results in method_results.items():\n",
    "        per_defect = obj_results.get(\"localization\", {}).get(\"per_defect_type\", {})\n",
    "        \n",
    "        for defect_type, auc_spro in per_defect.items():\n",
    "            val = auc_spro.get(str(FPR_LIMIT))\n",
    "            if val is not None:\n",
    "                if \"structural\" in defect_type:\n",
    "                    structural_vals.append(val)\n",
    "                elif \"logical\" in defect_type:\n",
    "                    logical_vals.append(val)\n",
    "    \n",
    "    if structural_vals:\n",
    "        print(f\"  Structural anomalies: {np.mean(structural_vals):.4f} (n={len(structural_vals)})\")\n",
    "    if logical_vals:\n",
    "        print(f\"  Logical anomalies:    {np.mean(logical_vals):.4f} (n={len(logical_vals)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image-Level Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze AUC-ROC for image-level classification\n",
    "print(\"Image-Level Classification (AUC-ROC)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method, method_results in results.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    \n",
    "    roc_values = []\n",
    "    \n",
    "    for obj_name, obj_results in method_results.items():\n",
    "        auc_roc = obj_results.get(\"classification\", {}).get(\"auc_roc\", {})\n",
    "        mean_roc = auc_roc.get(\"mean\")\n",
    "        \n",
    "        if mean_roc is not None:\n",
    "            roc_values.append(mean_roc)\n",
    "            print(f\"  {obj_name}: {mean_roc:.4f}\")\n",
    "    \n",
    "    if roc_values:\n",
    "        print(f\"  ---\")\n",
    "        print(f\"  Mean: {np.mean(roc_values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary findings\n",
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if results:\n",
    "    # 1. Best performing method\n",
    "    method_means = {}\n",
    "    for method, values in method_values.items():\n",
    "        if values:\n",
    "            method_means[method] = np.mean(list(values.values()))\n",
    "    \n",
    "    if method_means:\n",
    "        best_method = max(method_means, key=method_means.get)\n",
    "        print(f\"\\n1. BEST PERFORMING METHOD: {best_method}\")\n",
    "        print(f\"   Mean AUC-sPRO @ FPR={FPR_LIMIT}: {method_means[best_method]:.4f}\")\n",
    "    \n",
    "    # 2. Performance gap\n",
    "    if \"centralized\" in method_means and len(method_means) > 1:\n",
    "        print(f\"\\n2. FEDERATED vs CENTRALIZED GAP:\")\n",
    "        cent_mean = method_means[\"centralized\"]\n",
    "        \n",
    "        for method in [\"federated_iid\", \"federated_category\"]:\n",
    "            if method in method_means:\n",
    "                gap = method_means[method] - cent_mean\n",
    "                gap_pct = gap / cent_mean * 100\n",
    "                print(f\"   {method}: {gap_pct:+.1f}% ({gap:+.4f})\")\n",
    "    \n",
    "    # 3. Statistical significance\n",
    "    print(f\"\\n3. STATISTICAL SIGNIFICANCE:\")\n",
    "    for comp_key, comp_data in stats_analysis.get(\"comparisons\", {}).items():\n",
    "        p_val = comp_data.get(\"paired_t_test\", {}).get(\"p_value\", 1)\n",
    "        is_sig = \"YES\" if p_val < 0.05 else \"NO\"\n",
    "        print(f\"   {comp_key.replace('_vs_', ' vs ')}: {is_sig} (p={p_val:.4f})\")\n",
    "    \n",
    "    # 4. Effect sizes\n",
    "    print(f\"\\n4. EFFECT SIZES (Cohen's d):\")\n",
    "    for comp_key, comp_data in stats_analysis.get(\"comparisons\", {}).items():\n",
    "        d = comp_data.get(\"cohens_d\", 0)\n",
    "        if abs(d) < 0.2:\n",
    "            effect = \"negligible\"\n",
    "        elif abs(d) < 0.5:\n",
    "            effect = \"small\"\n",
    "        elif abs(d) < 0.8:\n",
    "            effect = \"medium\"\n",
    "        else:\n",
    "            effect = \"large\"\n",
    "        print(f\"   {comp_key.replace('_vs_', ' vs ')}: d={d:.2f} ({effect})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results summary\n",
    "summary = {\n",
    "    \"fpr_limit\": FPR_LIMIT,\n",
    "    \"method_means\": method_means if 'method_means' in dir() else {},\n",
    "    \"statistical_analysis\": stats_analysis,\n",
    "    \"comparison_table\": df.to_dict() if 'df' in dir() else {},\n",
    "}\n",
    "\n",
    "# Optionally save to file\n",
    "# output_path = project_root / \"outputs\" / \"analysis_summary.json\"\n",
    "# with open(output_path, \"w\") as f:\n",
    "#     json.dump(summary, f, indent=2)\n",
    "# print(f\"Summary saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
